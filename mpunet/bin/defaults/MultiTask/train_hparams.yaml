# This files stores hyperparameters for the building and fitting of a model
# in the models library.
#
# Components prefixed __CB are for keras callback hyperparamer settings. These
# will be passed as dictionaries stored in the 'callbacks' list in &FIT.

__CB_rlop: &RLOP
  # keras.ReduceLROnPlateau
  nickname: "rlop"
  class_name: "ReduceLROnPlateau"
  kwargs: {patience: 2, factor: 0.90, verbose: 1, monitor: "val_dice", mode: "max"}

__CB_tb: &TB
  # tensorboard
  nickname: "tb"
  class_name: "TensorBoard"
  kwargs: {log_dir: './tensorboard', profile_batch: 0}

__CB_mcp_clean: &MCP_CLEAN
  # Model checkpoint
  nickname: "mcp_clean"
  class_name: "ModelCheckPointClean"
  kwargs: {filepath: "./model/@epoch_{epoch:02d}_val_dice_{val_dice:.5f}.h5",
           monitor: "val_dice", save_best_only: true, save_weights_only: true,
           verbose: 1, mode: "max"}

__CB_es: &ES
  # Early stopping
  nickname: "es"
  class_name: "EarlyStopping"
  kwargs: {monitor: 'val_dice', min_delta: 0, patience: 15, verbose: 1 , mode: 'max'}

__CB_timer: &TIMER
  # Train timer callback
  nickname: "timer"
  class_name: "TrainTimer"
  pass_logger: True
  kwargs: {verbose: True}

__CB_csv: &CSV
  # keras.CSVLogger
  nickname: "csv"
  class_name: "CSVLogger"
  kwargs: {filename: "logs/training.csv", separator: ",", append: true}

tasks:
  task_names: ["task_1", "task_2"]
  hparam_files: ["task_1.yaml", "task_2.yaml"]

build: &BUILD
  #
  # Hyperparameters passed to the Model.build and __init__ methods
  #
  model_class_name: "MultiTaskUNet2D"
  complexity_factor: 2
  l1_reg: False
  l2_reg: False
  depth: 4
  shared_decoder: False

fit: &FIT
  #
  # Hyperparameters passed to the Trainer object
  #

  # Views
  views: 6
  noise_sd: 0.1
  intrp_style: 'iso_live'

  # On-the-fly augmentation?
  # Leave empty or delete entirely if not
  augmenters: [
    {cls_name: "Elastic2D",
     kwargs: {alpha: [0, 450], sigma: [20, 30], apply_prob: 0.333}}
  ]

  # Loss function
  loss: "SparseCategoricalCrossentropy"
  metrics: ["sparse_categorical_accuracy"]

  # Pass parameters to the loss function here, e.g. class weights
  # Note: class weights only supported by some loss functions!
  # Leave empty or remove field if loss takes no parameters
  loss_kwargs: {
      # class_weights: [0.2, 0.7, 2.1]
      # gamma: 0.3
  }
  class_weights: False  # Set to True to compute and overwrite class weights in loss_kwargs automatically

  # Optimization
  batch_size: 16
  n_epochs: 500
  verbose: true
  shuffle_batch_order: true
  optimizer: "Adam"
  optimizer_kwargs: {lr: 5.0e-05, decay: 0.0, beta_1: 0.9, beta_2: 0.999, epsilon: 1.0e-8}

  # Minimum fraction of image slices with FG labels in each mini-batch
  fg_batch_fraction: 0.50
  bg_value: 1pct

  # Normalization, using sklearn.preprocessing scalers
  # NOTE: Applied across full image volumes (after interpolation)
  # Options: MinMaxScaler, StandardScaler, MaxAbsScaler,
  #          RobustScaler, QuantileTransformer, Null
  scaler: "RobustScaler"

  # Callbacks
  callbacks: [*RLOP, *TB, *MCP_CLEAN, *ES, *TIMER, *CSV]

__VERSION__: Null
__BRANCH__: Null
__COMMIT__: Null
